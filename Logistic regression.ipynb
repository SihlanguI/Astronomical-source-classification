{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Algorithm <br >\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid Function**\n",
    "\n",
    "In order to map predicted values to probabilities, we use the sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities. Mathematically the sigmoid function is defined as follows:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_{\\theta}(x)= \\frac{1}{1 + e^{-\\theta x}}=S(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "**Note** <br>\n",
    "S(z) = output values between 0 and 1 (probabililty estimate)<br>\n",
    "z = input to the function <br>\n",
    "e = base of natural log <br>\n",
    "\n",
    "**The sigmoid graph**\n",
    "<img src=\"sigmoid.png\">\n",
    "\n",
    "\n",
    "**Decision Boundary**\n",
    "\n",
    "The sigmoid function returns probability score between 0 and 1. In order to map this to a discrete class (true/false), we select a threshold value above which we will classify values into class 1 and below which we classify values into class 2.<br>\n",
    "\n",
    "For example in the above plot we chose a threshold of 0.5 in order to classify the two classes. <br>\n",
    "\n",
    "* p $\\geq$ 0.5 , class 1\n",
    "* p < 0.5 , class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function** <br>\n",
    "A cost function's main purpose is to penalize bad choices for the parameters to be optimized and reward good ones. The cost function for logistic regression is written with logarithmic functions. An argument for using the log form of the cost function comes from the statistical derivation of the likelihood estimation for the probabilities.<br >\n",
    "\n",
    "Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for y=1 and one for y=0.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "J({\\theta}) = \\frac{1}{m} Cost(h_{\\theta}(x),y)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\\n",
    "    Cost(h_{\\theta}(x),y) = \n",
    "\\begin{cases}\n",
    "    -log(h_{\\theta}(x)) & \\text{if } y= 1\\\\\n",
    "    -log(1-h_{\\theta}(x))              & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, we can combine the two cases as follows:\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^m y.log(h_{\\theta}(x)) + (1-y)log(1-h_{\\theta}(x)) ] \n",
    "\\end{eqnarray}\n",
    "\n",
    "Multiplying by y and (1âˆ’y) in the above equation is a clever way that allows us use the same equation to solve for both y=1 and y=0 cases. If y=0, the first side cancels out. If y=1, the second side cancels out. In both cases we only perform the operation we need to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent** <br>\n",
    "\n",
    "To minimize the cost function we use gradient descent method.\n",
    "\n",
    "<img src=\"gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematics behind Gradiend descent:** <br >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpoose gradient descent is to find parameters that will minimize the cost function. The algorithm update the parameters as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta}J(\\theta) \n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\alpha$ is the learning rate. It determines the amount of time the algorithm will take to converge. If $\\alpha$ is too small the algorithm might take longer time to converge, and if is too large the algorithm might not converge. Refer to the clip that shows the concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying our cost function: <br>\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^m y.log(h_{\\theta}(x)) + (1-y)log(1-h_{\\theta}(x)) ] \n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First term:<br>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "log(h_{\\theta}(x)) = log(\\frac{1}{1+e^{-\\theta x}}) = -log(1+e^{-\\theta x})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second term: <br>\n",
    "\\begin{eqnarray}\n",
    "log(1 - h_{\\theta}(x)) = log(1 - \\frac{1}{1+e^{-\\theta x}}) = log(e^{-\\theta x}) - log(1+e^{-\\theta x}) = -\\theta x - log(1+e^{-\\theta x})\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the simplified terms into the cost function we get :<br>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m[-y.log(1+e^{-\\theta x}) + (1-y)(-\\theta x - log(1+e^{-\\theta x}))]\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which simplifies to: <br>\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y\\theta x -\\theta x - log(1+e^{-\\theta x})] = -\\frac{1}{m}\\sum_{i=1}^m [y\\theta x - log(1+e^{\\theta x})]\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the partial derivatives:<br>\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial }{\\partial \\theta} y\\theta x = yx\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial }{\\partial \\theta} log(1+e^{\\theta x}) = \\frac{xe^{\\theta x}}{1+e^{\\theta x}} = xh_{\\theta}(x)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Now we are ready to implement our Machine Learning algorithm.</span><sp>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
